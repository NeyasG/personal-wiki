[{"content":"ðŸ§ª Test hard. Test often If there is one thing I wish I learned earlier in my programming journey, it would be:\nDON\u0026rsquo;T. SKIP. TESTING.\nWell now I\u0026rsquo;m slightly older and perhaps slightly wiser, I\u0026rsquo;m starting to see the benefits of approaching a programming problem from a testing first workflow. Today in my work I was building out a metadata system to programmatically gather metadata from external sources (yeah okay\u0026hellip; it\u0026rsquo;s Excel ðŸ˜”) and parse them into a standardised format that could be applied to table comments and table tags within Unity Catalog.\nIn this post I\u0026rsquo;ll be diving into some intermediate pytest concepts that will save a lot of time and boiler plate in your code, and allow for you to rapidly add or tweak test cases on the fly.\nWith some simple examples!\nTesting is tiring (if done badly) If you\u0026rsquo;d have asked me when I first started writing tests what the most annoying part would be, chances are you\u0026rsquo;d get one of these answers:\nMaking the mock data or inputs to the test Manually having to write out many variations of a test case Fixtures A Pytest Fixture is the first important topic to understand. I\u0026rsquo;ll leave the detailed explanation for the docs, but in a nutshell, they allow you to specify functions that provide chunks of code that your test cases can access to perform various repetitive tasks.\nThis helps with a few things:\nProviding Data: Supplying consistent test data (like mock objects or configurations) to multiple tests. Managing Resources: Setting up resources (like database connections, temporary files, or running services) before a test and ensuring they are cleaned up afterwards, regardless of whether the test passes or fails. Reducing Boilerplate: Avoiding writing the same setup/teardown logic in multiple test functions. Let\u0026rsquo;s take a look at an example, of a simple multiplication function to start with:\n1 2 3 # A Simple function to test def multiply(a, b): return a * b Creating a basic pytest test would look like this:\n1 2 3 # A basic Pytest test def test_multiply(): assert multiply(3, 4) == 12 Now that\u0026rsquo;s great, but let\u0026rsquo;s introduce a more realistic scenario. Imagine our multiply function needs to write to a temporary file for some reason (very common when handling user data).\nIf we wrap a function with a @pytest.fixture decorator we can use the function to perform operations before each test:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import os import pytest # Defining a pytest fixture for opening and closing a file @pytest.fixture # This decorator declares the function as a pytest fixture def temp_output_path(): \u0026#34;\u0026#34;\u0026#34;Creates a temporary file path and ensures cleanup.\u0026#34;\u0026#34;\u0026#34; filepath = \u0026#39;temp_multiply_output.txt\u0026#39; # Remove file if it exists already if os.path.exists(filepath) os.remove(filepath) yield filepath # Provides filepath to the test # Teardown: Cleaning up the file after the test runs if os.path.exists(filepath): os.remove(filepath) else: print(f\u0026#34; TEARDOWN: temp_output_path (File not found, maybe test failed early?)\u0026#34;) Now, we\u0026rsquo;ve created a pytest fixture that will run before every test. Notice something interesting we are doing. We aren\u0026rsquo;t returning anything from the function, but rather using the keyword yield. This is important as return would end the function call and our teardown code would never run. So we\u0026rsquo;d be left with a temporary file path that never gets automatically cleaned up.\nUsing yield allows us to write teardown logic which will only run after the test completes.\nNow let\u0026rsquo;s incorporate this into a test:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # --- Test using our fixture --- def test_multiplication_with_file_output(temp_output_path): # Fixture is passed in a parameter \u0026#34;\u0026#34;\u0026#34; Tests multiplication, writes result to a temp file managed by a fixture, and verifies the file content. \u0026#34;\u0026#34;\u0026#34; result = multiply(3, 4) # 2. Use the file path from the fixture with open(temp_output_path, \u0026#34;w\u0026#34;) as f: f.write(str(result)) # 3. Verify the file contents with open(temp_output_path, \u0026#34;r\u0026#34;) as f: content = f.read() assert int(content) == 12 Looks great! This keeps our test code much cleaner, and allows us to reuse the fixture across multiple tests which interact with a temporary file.\nControlling Setup/Teardown Frequency with Scopes By default, the temp_output_path fixture we created runs its setup (checking/deleting the old file) and teardown (deleting the new file) for every single test that uses it. If this setup/teardown were time-consuming (like starting a database service), this would be inefficient.\nPytest allows us to control this using the scope argument in the @pytest.fixture decorator. Let\u0026rsquo;s change our temp_output_path fixture to have module scope. This means it will only set up once before the first test in the module that needs it runs, and tear down once after the last test in the module finishes.\nLet\u0026rsquo;s modify our fixture to create an entire folder, that all our tests can use temporarily:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import pytest import os @pytest.fixture(scope=\u0026#34;module\u0026#34;) # Defining the scope as \u0026#34;module\u0026#34; for the fixture def temp_output_dir(): \u0026#34;\u0026#34;\u0026#34;Creates a temporary directory once per module and ensures cleanup.\u0026#34;\u0026#34;\u0026#34; dirpath = \u0026#34;temp_module_output_dir\u0026#34; print(f\u0026#34;\\nSETUP: temp_output_dir (scope: module) - Creating directory: {dirpath}\u0026#34;) os.makedirs(dirpath, exist_ok=True) yield dirpath # Provide the directory path to the tests # Teardown: Clean up the directory after all tests in the module run print(f\u0026#34;TEARDOWN: temp_output_dir (scope: module) - Removing directory: {dirpath}\u0026#34;) # Be careful with rmtree in real code! if os.path.exists(dirpath): # Simple example cleanup: remove files first, then dir for item in os.listdir(dirpath): item_path = os.path.join(dirpath, item) if os.path.isfile(item_path): os.remove(item_path) os.rmdir(dirpath) print(\u0026#34;TEARDOWN: temp_output_dir finished.\u0026#34;) Now our fixture runs once at the very beginning of our tests. Specifically once per module (i.e., per .py file).\nAnd our test function will use this folder now:\n1 2 3 4 5 6 7 8 9 10 11 12 13 # --- Test Functions --- def test_multiplication_file_output_1(temp_output_dir): # Reminder: Fixture passed as parameter \u0026#34;\u0026#34;\u0026#34;Test using the module-scoped directory.\u0026#34;\u0026#34;\u0026#34; result = multiply(3, 4) filepath = os.path.join(temp_output_dir, \u0026#34;output.txt\u0026#34;) with open(filepath, \u0026#34;w\u0026#34;) as f: f.write(str(result)) with open(filepath, \u0026#34;r\u0026#34;) as f: content = f.read() assert int(content) == 12 Pytest offers different scopes to control how often a fixture is set up and torn down:\nfunction: Runs once per test function. class: Runs once per test class. module: Runs once per module (i.e., per .py file). session: Runs once per test session (i.e., when you run pytest). Handling Test Variations Cleanly with Parameterization Okay, now let\u0026rsquo;s tackle the second common pain point mentioned earlier: manually writing out many variations of the same test case. This is where parameterization comes in.\nImagine you want to test your multiply function with several different inputs: positive numbers, negative numbers, zero, etc. Without parameterization, you might write separate tests:\n1 2 3 4 5 6 7 8 9 # --- Repetitive Tests (What we want to avoid) --- def test_multiply_positive(): assert multiply(3, 4) == 12 def test_multiply_negative(): assert multiply(-2, 5) == -10 def test_multiply_zero(): assert multiply(6, 0) == 0 This is tedious and violates the DRY (Don\u0026rsquo;t Repeat Yourself) principle. Pytest\u0026rsquo;s parameterization lets you run the same test function multiple times with different arguments.\nYou use the @pytest.mark.parametrize decorator to achieve this. You provide it with:\nA string containing the names of the arguments the test function will receive (comma-separated). A list of tuples (or lists), where each tuple represents one set of arguments to pass to the test function for one run. Let\u0026rsquo;s add a parameterized test to our example file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # --- Parameterized Test --- @pytest.mark.parametrize( \u0026#34;a, b, expected_product\u0026#34;, # Argument names for the test function [ (2, 5, 10), # Test case 1: a=2, b=5, expected_product=10 (-3, 6, -18), # Test case 2: a=-3, b=6, expected_product=-18 (0, 100, 0), # Test case 3: a=0, b=100, expected_product=0 (7, -7, -49), # Test case 4: a=7, b=-7, expected_product=-49 ], ids=[\u0026#34;pos*pos\u0026#34;, \u0026#34;neg*pos\u0026#34;, \u0026#34;zero*pos\u0026#34;, \u0026#34;pos*neg\u0026#34;] # Optional: Custom IDs for test runs ) def test_parametrized_multiplication(a, b, expected_product, temp_output_dir): # Pass parameterized arguments \u0026#34;\u0026#34;\u0026#34;Tests multiplication with multiple input sets using parameterization.\u0026#34;\u0026#34;\u0026#34; # Note: We can still use fixtures alongside parameterization! print(f\u0026#34;\\n TEST: test_parametrized_multiplication [{a=}, {b=}] (Dir: {temp_output_dir})\u0026#34;) result = multiply(a, b) assert result == expected_product # Optional: Could also write to a unique file per parameter set in temp_output_dir # filepath = os.path.join(temp_output_dir, f\u0026#34;output_param_{a}_{b}.txt\u0026#34;) # with open(filepath, \u0026#34;w\u0026#34;) as f: f.write(str(result)) @pytest.mark.parametrize(\u0026quot;a, b, expected_product\u0026quot;, ...) tells pytest that this test function takes three arguments (a, b, expected_product) that will be parameterized. The list [(2, 5, 10), (-3, 6, -18), ...] provides the different sets of values for these arguments.\npytest will run the test_parametrized_multiplication function four times, once for each tuple in the list: Run 1: a=2, b=5, expected_product=10 Run 2: a=-3, b=6, expected_product=-18 Run 3: a=0, b=100, expected_product=0 Run 4: a=7, b=-7, expected_product=-49 The ids list provides clearer names for each parameterized run in the pytest output (e.g., test_parametrized_multiplication[pos*pos]). Notice that we can still request fixtures (like temp_output_dir) in a parameterized test. The fixture\u0026rsquo;s scope rules still apply (e.g., temp_output_dir is set up only once for the module, even though this test function runs four times).\nParameterization makes it incredibly easy to add new test cases â€“ just add another tuple to the list! This significantly reduces boilerplate code and makes your tests more comprehensive and maintainable.\nBut Wait\u0026hellip; There\u0026rsquo;s MORE (Parameterization) \"Turtles all the way down\" - It just keeps going.\nSometimes, you want a fixture to provide slightly different setup or data depending on the test. You can achieve this by passing parameters to the fixture when requesting it in the test function and accessing these parameters within the fixture using request.param.\nLet\u0026rsquo;s create a fixture and parametrize the fixture itself (not the test case) accept a parameter that determines the numbers it yields:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # --- Parameterizable Fixture --- @pytest.fixture(params=[ {\u0026#34;x\u0026#34;: 3, \u0026#34;y\u0026#34;: 4, \u0026#34;id\u0026#34;: \u0026#34;standard\u0026#34;}, # Parameter set 1 {\u0026#34;x\u0026#34;: 10, \u0026#34;y\u0026#34;: 2, \u0026#34;id\u0026#34;: \u0026#34;large_x\u0026#34;}, # Parameter set 2 pytest.param({\u0026#34;x\u0026#34;: -5, \u0026#34;y\u0026#34;: -5, \u0026#34;id\u0026#34;: \u0026#34;negatives\u0026#34;}, marks=pytest.mark.skip) # Parameter set 3 (skipped) ]) def parameterized_operands(request): \u0026#34;\u0026#34;\u0026#34;Provides different pairs of numbers based on fixture parameterization.\u0026#34;\u0026#34;\u0026#34; param_data = request.param # Access the current parameter set yield {\u0026#34;x\u0026#34;: param_data[\u0026#34;x\u0026#34;], \u0026#34;y\u0026#34;: param_data[\u0026#34;y\u0026#34;]} # Yield only the needed data # --- Module-scoped fixture (unchanged) --- # ... # --- Test using the parameterized fixture --- def test_with_parameterized_fixture(parameterized_operands, temp_output_dir): \u0026#34;\u0026#34;\u0026#34;This test will run multiple times, once for each fixture parameter.\u0026#34;\u0026#34;\u0026#34; operands = parameterized_operands # Get the data yielded by the fixture for this run print(f\u0026#34;TEST: test_with_parameterized_fixture (Operands: {operands}, Dir: {temp_output_dir})\u0026#34;) result = multiply(operands[\u0026#34;x\u0026#34;], operands[\u0026#34;y\u0026#34;]) expected = operands[\u0026#34;x\u0026#34;] * operands[\u0026#34;y\u0026#34;] assert result == expected # Example: Write to a file named after the fixture param ID filepath = os.path.join(temp_output_dir, f\u0026#34;output_{operands[\u0026#39;x\u0026#39;]}_{operands[\u0026#39;y\u0026#39;]}.txt\u0026#34;) with open(filepath, \u0026#34;w\u0026#34;) as f: f.write(str(result)) Let\u0026rsquo;s walk through what we just did:\nWe add params=[...] to the @pytest.fixture decorator for parameterized_operands. Each item in the params list will cause any test requesting this fixture to run once per parameter. Inside the fixture, request.param holds the current parameter value for that specific run (e.g., {\u0026quot;x\u0026quot;: 3, \u0026quot;y\u0026quot;: 4, \u0026quot;id\u0026quot;: \u0026quot;standard\u0026quot;} on the first run). We use request.param['id'] in the print statements and request.param['x'], request.param['y'] to yield the correct data. The test_with_parameterized_fixture function requests parameterized_operands. Pytest sees the fixture is parameterized and runs the test twice (once for each non-skipped parameter set in the fixture definition). The third parameter set is marked with pytest.mark.skip. The temp_output_dir fixture (module scope) is still set up only once. Wrapping up Once I got my head around these concepts, writing tests became much less daunting, and the tests I was writing started to have the speed and agility required for me to actually go and write them (shout-out to all the lazy developers out there).\nThere is a lot more to delve into when it comes to Pytest. The future me will likely write about some more complex topics, like running Pytest both locally and also in Databricks Connect on a cluster in the cloud. And perhaps even,\nIs there such a thing as too much Testing?\nHint. Yes there is.\nBut this post has already grown too large so\u0026hellip; Thanks for reading!\n","date":"2025-04-28T22:21:04Z","permalink":"https://neyasg.github.io/personal-wiki/p/deep-dive-into-pytest-fixtures-scopes-parameterization-and-testing-spark-remotely/","title":"Deep Dive into Pytest: Fixtures, Scopes, Parameterization, and Testing Spark Remotely"},{"content":" Welcome to my personal blog! This space serves as my personal wiki and progress tracker as I dive into the world of Data Engineering, and software development. Stay a while and listen!\nA little bit about me Graduating in Mechanical Engineering from Heriot-Watt University in 2018 may lead some down the path of industry, working their way to a chartered engineering certification and paving the way for a career in companies building bridges, cars, or machines. My journey however started off quite differently, with a stint in the professional video game industry.\nPlaying for an eSports team for a living was quite the unorthodox career, but it taught me a few things which I found useful now I\u0026rsquo;m working in what I would previously call a \u0026ldquo;boring\u0026rdquo; 9-5. The non-stop competitive gauntlet I was immersed in daily solved a lot of what I was lacking in University. Drive. So after retiring from competitive eSports and stumbling into the world of data engineering by almost chance (A story for another post), I find myself using that drive to tickle the problem solving part of my brain, building data pipelines, untangling strange behaviour and solving data puzzles.\nWhat\u0026rsquo;s in the pipeline? Expect posts covering topics on Databricks, AWS, Apache Spark, Terraform and any other tools and platforms I encounter as I build up my experience. I\u0026rsquo;ll be documenting my personal projects, challenges and all, the first of which is a simulated music streaming application (think Spotify) ingesting data in real time using Apache Kafka and Spark structured streaming to create a live dashboard.\nFor anyone stumbling on this blog, whether that be fellow Data Engineers and devs, or LinkedIn connections or if I\u0026rsquo;m lucky, potential employers, thank you for reading, and I hope you\u0026rsquo;ve found something useful while here!\n","date":"2025-04-27T00:00:00Z","image":"https://neyasg.github.io/personal-wiki/p/from-pro-gamer-to-data-engineer/cover_hu_f5f7984e3424221a.jpg","permalink":"https://neyasg.github.io/personal-wiki/p/from-pro-gamer-to-data-engineer/","title":"From Pro Gamer to Data Engineer"}]